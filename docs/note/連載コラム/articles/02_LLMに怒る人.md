# #2 LLMに怒る人が理解していない本質

## AIにイラッとした経験、ありますよね

ChatGPTに「さっきも説明したよね？」とイラッとしたこと、ありませんか？

何度も同じミスを繰り返すAIに、「このAI、ちょっと頭悪いんじゃないか」と思ったこと。

「ちゃんと指示したのに、なんで理解してくれないんだ！」と怒りたくなったこと。

その感情、よくわかります。

でも、**その怒り、実はContext Control訓練の最大の障害**なんです。

## LLMは「性格」を持っていない

まず、これを理解してください。

**LLMには「性格」も「意図」も「記憶」もありません。**

「え、でも会話してくれるじゃん」と思いますよね。確かに、会話形式で返してくれます。まるで人間のように。

でも、それは「そう見えるように学習された」だけです。

LLMの本質は、**「渡された文脈から、次に来る語を確率で選ぶ装置」**です。

それ以上でも、それ以下でもありません。

あなたが「こんにちは」と入力したら、学習データから「こんにちは。何かお手伝いできますか？」という文言が高確率で続くと判断して、それを出力します。

「親切だな」と思いましたか？ それは幻想です。LLMはただ、統計的に「こう返すと会話が続く確率が高い」と判断しただけです。

## なぜ擬人化してしまうのか

人間は、相手に「性格」があると感じるように進化してきました。

赤ちゃんは、ぬいぐるみにも名前をつけ、「この子は優しい」と言います。ロボット掃除機に「お疲れ様」と声をかけます。

これは人間の本能です。

そして、ChatGPTやClaudeのような会話形式のUIは、その本能を刺激します。まるで「向こうに誰かいる」かのように感じさせます。

**しかし、擬人化は、Context Controlの妨げになります。**

なぜなら、あなたが「このAI、性格悪いな」と思っている限り、本質に向き合えないからです。

本質とは、**「あなたの与えた文脈が不十分だった」**という事実です。

## Context Controlの原則：AIの出力の品質 = あなたの入力の質

これを覚えてください。

**AIの出力の品質 = あなたの入力の質**

これが、**Context Control Technologyの最も基本的な原則**です。

LLMは「文脈依存装置」です。あなたが与えた文脈（コンテクスト）にのみ依存して、出力を生成します。

文脈が曖昧なら、出力も曖昧です。
文脈が明確なら、出力も明確です。

問題は「AI」ではなく、**「あなたの文脈設計」**なのです。

### 比喩1: 電卓に怒りますか？

電卓に「2+2=」と入力して「5」と出たら、あなたは電卓に怒りますか？

いいえ、怒りませんよね。

「あれ、入力ミスかな」と考えて、もう一度「2+2=」と入力し直すはずです。

LLMも同じです。

間違った文脈を与えれば、間違った出力が返ります。**それだけ**です。

AIに怒るのは、電卓に怒るのと同じくらい無意味なのです。

### 比喩2: レストランでの注文

レストランで「何か美味しいもの」と注文して、気に入らないものが来たら、あなたは店を責めますか？

いいえ、ですよね。

「あ、もっと具体的に言えばよかった」と思うはずです。

「魚料理で、あっさりした味付けで、前菜付きで」と明確に注文すれば、期待に近いものが来ます。

**あなたの指示（文脈）が曖昧だった**だけです。

LLMも同じ。**明確な文脈を渡せば、明確な結果が返る**のです。

## 怒る対象を転換する：AIから文脈設計へ

では、どうすればいいのか。

**怒る対象を「AI」から「自分の文脈設計」に転換してください。**

AIが期待と違う出力をしたとき、こう自問してください：

「自分の与えた文脈は明確だったか？」

「何が足りなかったのか？」

「どう改善すれば、期待通りの出力が得られるか？」

この姿勢が、**Context Control訓練の第一歩**です。

そして、この訓練を極めるために、vibeCodingでは「プログラミング」を題材にしています。

なぜなら、プログラミングは**「一文字でも間違えればエラー」**だからです。

曖昧さが一切許されない。AIに感情投影している余裕はない。ただひたすら、「文脈（コード、指示、要件定義）を正しく設計する」ことに向き合わざるを得ない。

この厳しさが、Context Controlの完璧な訓練になるのです。

## 今すぐ試せる：Context Controlの実験

理論はわかった。でも、本当にそうなのか？

今すぐ実験してみましょう。

### 実践1: 同じ質問を2つの方法で試す

ChatGPTまたはClaudeを開いて、まずこう質問してください：

❌ **「何かアイデアください」**

どうなりましたか？ おそらく、汎用的で当たり障りのないアイデアが返ってきたはずです。

次に、こう質問してみてください：

✅ **「30代ビジネスパーソン向けの、週末に始められる副業アイデアを3つ、それぞれの初期投資額と予想月収を含めて教えてください」**

どうでしょう？ 期待に近い、具体的なアイデアが返ってきませんでしたか？

**これが、文脈の明確性が結果を左右する、という証拠です。**

### 実践2: 「AIが間違えた」と思ったら、文脈を見直す習慣

今日から、AIが期待と違う出力をしたとき、こう自問してください：

「自分の与えた文脈は明確だったか？」

この習慣が、Context Control訓練の第一歩です。

## まとめ：文脈をコントロールできる者が、AI駆動開発を制する

LLMに怒るのは、電卓に怒るのと同じくらい無意味です。

怒りの矛先を「AI」から「**自分の文脈設計**」に向けましょう。

**これがContext Control Technology訓練の第一歩です。**

そして、プログラミングという「一文字でも間違えればエラー」な題材で、この訓練が極まります。

文脈をコントロールできる者が、AI駆動開発を制します。

感情投影をやめ、文脈設計に向き合う。

この転換ができたとき、あなたのAI活用は、次のレベルに進みます。

---

**次回予告**

では、具体的にLLMはどうやって「間違える」のでしょうか？

実は、あなたが何気なく与えた情報が、AIの文脈を「汚染」していることがあるんです。

次回は「**コンテクストが汚染される瞬間**」を解説します。

---

*この記事は、企業研修として1人あたり180万円の価値を持つVibe Coder Bootcampのエッセンスを凝縮してお届けしています。*

*参考: [意味ないからLLMに怒るな、問い詰めるな](https://note.com/rsensui/n/n41598ed615ae)*
